{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d97811f6",
   "metadata": {},
   "source": [
    "# Pipeline de preparación de datos para fine-tuning con LoRA\n",
    "\n",
    "Este notebook documentado contiene el pipeline completo para cargar, limpiar y preparar datos con el objetivo de realizar un fine-tuning de modelos de lenguaje usando la técnica adaptaciones LoRA. Cada bloque de código va precedido por una celda Markdown que explica el propósito, qué hace y por qué es importante (justificación), además de consideraciones prácticas.\n",
    "\n",
    "Objetivos generales:\n",
    "\n",
    "- Evitar *data leakage* mediante divisiones correctas y control de información temporal.\n",
    "- Asegurar la calidad de los datos revisando su estructura interna y sondeando todos los datos.\n",
    "- Formatear y tokenizar los datos en el formato requerido por el modelo y por LoRA/PEFT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc63023",
   "metadata": {},
   "source": [
    "### Paso 1: Configuración e importación de librerías\n",
    "\n",
    "**Objetivo:** Este bloque importa las librerías y configura el entorno necesario para el pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c96e2cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ce0e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_TRAIN = \"fine_tuning_train_500_dataset.json\"\n",
    "FILE_VAL = \"fine_tuning_50_test_dataset.json\"\n",
    "\n",
    "\n",
    "CLASS_NO_DUPLICATE = \"✔️ No duplicates detected\"\n",
    "CLASS_DUPLICATE_PREFIX = \"❗ Issue may be repeated or similar to UCM\"\n",
    "\n",
    "TEMPLATE_PARTS = [\n",
    "    \"Eres un meticuloso analista de datos experto en Jira.\",\n",
    "    \"**ISSUE EN ANÁLISIS AHORA:**\",\n",
    "    \"---\",\n",
    "    \"### PROCESO DE RAZONAMIENTO Y REGLAS\",\n",
    "    \"**Paso 1: Tarea Única - Detección de Duplicados.**\",\n",
    "    \"**Paso 2: Generar el Payload Combinado.**\",\n",
    "    \"---\",\n",
    "    \"### DATOS PARA TU ANÁLISIS\",\n",
    "    \"**1. CURRENT ISSUE (El que estás analizando ahora):**\",\n",
    "    \"**2. POTENTIALLY SIMILAR ISSUES (Encontrados en la memoria vectorial):**\",\n",
    "    \"---\",\n",
    "    \"### REGLAS DE PAYLOAD\",\n",
    "    \"* **FORMATO:**\",\n",
    "    \"* **PROHIBICIÓN ABSOLUTA:**\",\n",
    "    \"* **CAMPOS PROHIBIDOS:**\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ef5aaa",
   "metadata": {},
   "source": [
    "**Paso 2: Eliminación de duplicados**\n",
    "\n",
    "- **Objetivo:** Detectar y eliminar duplicados exactos y redundancias contextuales en el dataset para evitar `data leakage` y sobreajuste antes de dividir los datos en `train`/`val`/`test`.\n",
    "\n",
    "- **Resumen (qué hace el código):**\n",
    "  - `get_unique_key(item)`: crea una clave hashable `(input, output)` para detectar duplicados exactos.\n",
    "  - `validate_input_template(input_text)`: verifica que el prompt siga la estructura estática definida en `TEMPLATE_PARTS`.\n",
    "  - `detect_context_redundancy(input_text)`: analiza la sección `POTENTIALLY SIMILAR ISSUES` buscando pares `Summary/Description` repetidos dentro del mismo prompt.\n",
    "  - `process_dataset(file_path)`: orquesta verificaciones (integridad JSON, estructura, plantilla, redundancia y duplicados internos) y devuelve `valid_data` listo para análisis posterior.\n",
    "  - `analyze_data_distribution(data, file_name)`: calcula la distribución de clases (`No Duplicates` vs `Duplicates`) e imprime alertas si hay desbalance.\n",
    "  - `check_cross_contamination(train_data, val_data)`: compara `train` y `val` para detectar ejemplos idénticos (fuga de datos).\n",
    "\n",
    "- **Salida y mensajes esperados:**\n",
    "  - Mensajes informativos como `PROCESANDO: <file>` y resúmenes `Resultados del Análisis:`.\n",
    "  - Advertencias `⚠️` para errores de plantilla, ítems con contexto redundante, o duplicados internos.\n",
    "  - `CRÍTICO` si se detecta contaminación cruzada entre `train` y `val`.\n",
    "\n",
    "- **Justificación:** Los duplicados y la redundancia en el contexto inflan el rendimiento aparente y permiten que el modelo memorice ejemplos en lugar de generalizar; eliminarlos o marcarlos reduce el overfitting y evita métricas optimistas en validación.\n",
    "\n",
    "- **Consideraciones y recomendaciones:**\n",
    "  - Mantener reproducibilidad: fijar semillas y versionado de librerías antes de procesar.\n",
    "  - No eliminar automáticamente near-duplicates sin revisión humana; considerar hashing difuso o reglas de umbral si procede.\n",
    "  - Revisar y corregir ejemplos con errores de `TEMPLATE_PARTS` antes de descartarlos.\n",
    "  - Si el dataset queda desbalanceado, aplicar `upsampling` o `downsampling` según corresponda.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5922b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_key(item: Dict) -> Tuple[str, str]:\n",
    "    \"\"\"Genera una tupla hashable (input, output) para detectar duplicados exactos.\"\"\"\n",
    "    try:\n",
    "        input_str = item['input']\n",
    "        output_str = json.dumps(item['output'], sort_keys=True, ensure_ascii=False)\n",
    "        return (input_str, output_str)\n",
    "    except (KeyError, TypeError):\n",
    "        return None\n",
    "\n",
    "def validate_input_template(input_text: str) -> bool:\n",
    "    \"\"\"Verifica que el prompt cumpla con la estructura estática requerida.\"\"\"\n",
    "    current_position = 0\n",
    "    for part in TEMPLATE_PARTS:\n",
    "        found_position = input_text.find(part, current_position)\n",
    "        if found_position == -1:\n",
    "            return False\n",
    "        current_position = found_position + len(part)\n",
    "    return True\n",
    "\n",
    "def detect_context_redundancy(input_text: str) -> int:\n",
    "    \"\"\"\n",
    "    NUEVA FUNCIÓN: Analiza la sección 'POTENTIALLY SIMILAR ISSUES' \n",
    "    para detectar si hay Summary/Description repetidos dentro del mismo prompt.\n",
    "    \n",
    "    Retorna: Número de duplicados encontrados en este prompt específico.\n",
    "    \"\"\"\n",
    "   \n",
    "    start_marker = \"**2. POTENTIALLY SIMILAR ISSUES (Encontrados en la memoria vectorial):**\"\n",
    "    end_marker = \"---\" \n",
    "    \n",
    "    start_idx = input_text.find(start_marker)\n",
    "    if start_idx == -1: return 0\n",
    "    \n",
    "    \n",
    "    sub_text = input_text[start_idx + len(start_marker):]\n",
    "    \n",
    "    \n",
    "    end_idx = sub_text.find(end_marker)\n",
    "    if end_idx != -1:\n",
    "        sub_text = sub_text[:end_idx]\n",
    "        \n",
    "    pattern = re.compile(r\"Summary:\\s*(.+?)\\n\\s*Description:\\s*(.+?)(?=\\n- ISSUE|\\n\\n|\\n---|$)\", re.IGNORECASE)\n",
    "    \n",
    "    matches = pattern.findall(sub_text)\n",
    "    \n",
    "    seen_content = set()\n",
    "    redundancy_count = 0\n",
    "    \n",
    "    for summary, description in matches:\n",
    "        \n",
    "        clean_pair = (summary.strip(), description.strip())\n",
    "        \n",
    "        if clean_pair in seen_content:\n",
    "            redundancy_count += 1\n",
    "        else:\n",
    "            seen_content.add(clean_pair)\n",
    "            \n",
    "    return redundancy_count\n",
    "\n",
    "def analyze_data_distribution(data: List[Dict], file_name: str):\n",
    "    \"\"\"Analiza balance de clases (Duplicado vs No Duplicado).\"\"\"\n",
    "    print(f\"\\n  [Distribución] Análisis del archivo: {os.path.basename(file_name)}\")\n",
    "    counts = {\"no_duplicate\": 0, \"duplicate\": 0, \"unknown\": 0}\n",
    "    \n",
    "    for item in data:\n",
    "        try:\n",
    "            value = item['output']['customfield_10602']\n",
    "            if value.strip() == CLASS_NO_DUPLICATE:\n",
    "                counts[\"no_duplicate\"] += 1\n",
    "            elif value.strip().startswith(CLASS_DUPLICATE_PREFIX):\n",
    "                counts[\"duplicate\"] += 1\n",
    "            else:\n",
    "                counts[\"unknown\"] += 1\n",
    "        except (KeyError, TypeError):\n",
    "            counts[\"unknown\"] += 1 \n",
    "\n",
    "    total = len(data)\n",
    "    if total == 0: return\n",
    "\n",
    "    p_no_dup = (counts[\"no_duplicate\"] / total) * 100\n",
    "    p_dup = (counts[\"duplicate\"] / total) * 100\n",
    "    \n",
    "    print(f\"   - Total ejemplos: {total}\")\n",
    "    print(f\"   - 'No Duplicates': {counts['no_duplicate']} ({p_no_dup:.1f}%)\")\n",
    "    print(f\"   - 'Duplicates':    {counts['duplicate']} ({p_dup:.1f}%)\")\n",
    "    \n",
    "    if counts[\"unknown\"] > 0:\n",
    "        print(f\"   - Formato desconocido: {counts['unknown']}\")\n",
    "\n",
    "    if not (0.4 <= (p_no_dup / 100) <= 0.6):\n",
    "         print(\"   - ALERTA: Dataset desbalanceado. Recomendado técnica de upsampling/downsampling.\")\n",
    "    else:\n",
    "         print(\"   - ✅ Dataset balanceado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942e4af9",
   "metadata": {},
   "source": [
    "**Paso 3: Carga y validación inicial de datos**\n",
    "\n",
    "- **Objetivo:** Cargar el archivo fuente (`JSON`, `CSV` o `Excel`) y ejecutar las comprobaciones iniciales necesarias para garantizar que los datos están en el formato esperado antes de cualquier preparación adicional o división en `train/val/test`.\n",
    "\n",
    "- **Qué hace el código que sigue (resumen):**\n",
    "  - `process_dataset(file_path)`: función orquestadora que realiza:\n",
    "\n",
    "- **Entradas esperadas:**\n",
    "  - `file_path` (string): ruta a un archivo JSON que contiene una lista de ejemplos con la forma `{\"input\": <str>, \"output\": <dict>}`.\n",
    "    4) Detección de redundancia contextual dentro de la sección `POTENTIALLY SIMILAR ISSUES` (`detect_context_redundancy`).\n",
    "- **Salidas y efectos:**\n",
    "  - Retorna `valid_data`: lista de ítems que pasaron las comprobaciones básicas (lista de `dict`).\n",
    "  - Imprime mensajes informativos y alertas: `PROCESANDO`, `Resultados del Análisis`.\n",
    "  - Actualiza un log interno en memoria (`issues_log`) con conteos de errores de estructura, errores de plantilla, duplicados internos y redundancias contextuales.\n",
    "\n",
    "---  - Si se pretende usar formatos distintos a JSON, añadir adaptadores de lectura (p. ej. `pandas.read_csv`) antes de llamar a `process_dataset` o extender la función para soportarlos.  - Mantener backups de los archivos originales para poder revertir filtrados o correcciones manuales.  - Ejecutar `process_dataset` en local y revisar las primeras advertencias (`template_errors`) antes de borrar datos automáticamente.- **Recomendaciones prácticas:**- **Justificación:** Centralizar la carga y las comprobaciones evita que datos malformados contaminen etapas posteriores (tokenización, entrenamiento), y permite detectar fugas de información o memorias innecesarias antes de entrenar.  - Si se detecta `context_redundancy`, puede indicar que la sección de `POTENTIALLY SIMILAR ISSUES` contiene ejemplos repetidos y conviene depurarlos para evitar sesgos en RAG/recuperación.  - Plantillas de prompt que no encajan con `TEMPLATE_PARTS` se cuentan como `template_errors` (revisar y corregir manualmente si procede).  - Ítems con estructura distinta a `{'input','output'}` son contabilizados y saltados.  - Archivo inexistente o JSON corrupto (se captura y se informa).- **Errores y condiciones a vigilar:**\n",
    "**Justificación / razones:** Justificación: centralizar y documentar la carga de datos ayuda a comprobar formatos, detectar encabezados incorrectos y prevenir fugas de información (data leakage) durante posteriores pasos de preparación.\n",
    "\n",
    "**Consideraciones:** Consideraciones: revisar que las transformaciones no introduzcan información del futuro, anotar supuestos, y mantener reproducibilidad (semillas, versiones de librerías).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d1c021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(file_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Ejecuta todas las verificaciones sobre un archivo:\n",
    "    1. Integridad JSON\n",
    "    2. Estructura de campos\n",
    "    3. Duplicados internos (Data Leakage intra-set)\n",
    "    4. Validación de Plantilla (Prompt Template)\n",
    "    5. Redundancia de Contexto (RAG repetition)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESANDO: {file_path}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: Archivo no encontrado.\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: JSON corrupto o ilegible. {e}\")\n",
    "        return None\n",
    "\n",
    "    issues_log = {\n",
    "        \"structure_errors\": 0,\n",
    "        \"template_errors\": 0,\n",
    "        \"internal_duplicates\": 0,\n",
    "        \"context_redundancy_items\": 0 \n",
    "    }\n",
    "    \n",
    "    valid_data = []\n",
    "    \n",
    "\n",
    "    for i, item in enumerate(data):\n",
    "    \n",
    "        if set(item.keys()) != {'input', 'output'}:\n",
    "            issues_log[\"structure_errors\"] += 1\n",
    "            continue\n",
    "            \n",
    "        input_str = item.get('input', \"\")\n",
    "        \n",
    "\n",
    "        if not validate_input_template(input_str):\n",
    "            issues_log[\"template_errors\"] += 1\n",
    "            if issues_log[\"template_errors\"] == 1:\n",
    "                print(f\"Ejemplo de error de plantilla en índice {i}.\")\n",
    "        \n",
    "\n",
    "        redundancy_count = detect_context_redundancy(input_str)\n",
    "        if redundancy_count > 0:\n",
    "            issues_log[\"context_redundancy_items\"] += 1\n",
    "            print(f\"Ítem {i} tiene {redundancy_count} issues repetidos en su contexto.\") \n",
    "\n",
    "        valid_data.append(item)\n",
    "\n",
    "    unique_set = set()\n",
    "    for item in valid_data:\n",
    "        key = get_unique_key(item)\n",
    "        if key:\n",
    "            if key in unique_set:\n",
    "                issues_log[\"internal_duplicates\"] += 1\n",
    "            else:\n",
    "                unique_set.add(key)\n",
    "    \n",
    "    print(f\"Resultados del Análisis:\")\n",
    "    if sum(issues_log.values()) == 0:\n",
    "        print(\"Estructura, Template, Ningún error de duplicados internos. (Limpieza correcta)\")\n",
    "    else:\n",
    "        if issues_log[\"structure_errors\"]: print(f\"Errores de estructura JSON: {issues_log['structure_errors']}\")\n",
    "        if issues_log[\"template_errors\"]: print(f\"Errores en plantilla de prompt: {issues_log['template_errors']}\")\n",
    "        if issues_log[\"internal_duplicates\"]: print(f\"Duplicados exactos internos: {issues_log['internal_duplicates']} (Deben eliminarse)\")\n",
    "        if issues_log[\"context_redundancy_items\"]: \n",
    "            print(f\"Items con contexto redundante: {issues_log['context_redundancy_items']}\")\n",
    "            print(f\"(Esto significa que en 'POTENTIALLY SIMILAR ISSUES' aparecen issues repetidos)\")\n",
    "\n",
    "    analyze_data_distribution(valid_data, file_path)\n",
    "    \n",
    "    return valid_data\n",
    "\n",
    "def check_cross_contamination(train_data, val_data):\n",
    "    \"\"\"Verifica si hay datos de validación filtrados en entrenamiento.\"\"\"\n",
    "    print(\"\\n Verificando Fuga de Datos (Train vs Validation)...\")\n",
    "    if not train_data or not val_data:\n",
    "        print(\"No se puede verificar, faltan datos.\")\n",
    "        return\n",
    "\n",
    "    train_hashes = {get_unique_key(item) for item in train_data if get_unique_key(item)}\n",
    "    leakage_count = 0\n",
    "    \n",
    "    for item in val_data:\n",
    "        if get_unique_key(item) in train_hashes:\n",
    "            leakage_count += 1\n",
    "            \n",
    "    if leakage_count > 0:\n",
    "        print(f\"CRÍTICO: Se encontraron {leakage_count} ejemplos de validación dentro de train.\")\n",
    "    else:\n",
    "        print(\"Limpio. No hay contaminación cruzada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d654fd2f",
   "metadata": {},
   "source": [
    "**Ejecución: Carga, validación y verificación de fuga de datos**\n",
    "\n",
    "Este bloque ejecuta las comprobaciones completas del pipeline sobre los archivos fuente y, si ambos se procesan correctamente, verifica si hay ejemplos idénticos entre `train` y `val` (fuga de datos).\n",
    "\n",
    "- **Qué hace exactamente:**\n",
    "  1. `train_dataset = process_dataset(FILE_TRAIN)` -> Lee `FILE_TRAIN`, valida estructura, plantilla y duplicados internos, y retorna `valid_data` (lista de `dict`) o `None` si hay un error crítico.\n",
    "  2. `val_dataset = process_dataset(FILE_VAL)` -> Igual verificación para el conjunto de validación.\n",
    "  3. `if train_dataset and val_dataset: check_cross_contamination(train_dataset, val_dataset)` -> Si ambos conjuntos se cargaron correctamente, calcula hashes únicos por ejemplo y cuenta cuántos de los ejemplos de `val` aparecen en `train`. Imprime un mensaje crítico si se detecta contaminación.\n",
    "\n",
    "- **Salidas esperadas en la consola:**\n",
    "  - Mensajes `PROCESANDO: <file>` mientras se leen los ficheros.\n",
    "  - Resumen `Resultados del Análisis:` con conteos de `structure_errors`, `template_errors`, `internal_duplicates` y `context_redundancy_items`.\n",
    "  - Si hay contaminación cruzada: `CRÍTICO: Se encontraron <n> ejemplos de validación dentro de train.`\n",
    "  - Si no hay contaminación: `Limpio. No hay contaminación cruzada.`\n",
    "\n",
    "- **Acciones recomendadas según resultado:**\n",
    "  - Si `process_dataset` devuelve `None` para cualquiera de los archivos: revisar el error impreso (archivo no encontrado o JSON corrupto) y corregir el origen antes de continuar.\n",
    "  - Si hay `template_errors`: abrir algunos ejemplos reportados y corregir manualmente la sección del prompt o ajustar `TEMPLATE_PARTS` si corresponde.\n",
    "  - Si hay `internal_duplicates`: eliminar o consolidar ejemplos duplicados antes de entrenar para evitar sesgos.\n",
    "  - Si se detecta contaminación cruzada (`leakage_count > 0`): auditar los ejemplos coincidentes (por ejemplo, buscar `get_unique_key(item)` para los ítems dañinos) y eliminar/reestruturar los datos para garantizar independencia entre `train` y `val`.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b7975f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROCESANDO: fine_tuning_train_500_dataset.json\n",
      "============================================================\n",
      "Resultados del Análisis:\n",
      "Estructura, Template, Ningún error de duplicados internos. (Limpieza correcta)\n",
      "\n",
      "  [Distribución] Análisis del archivo: fine_tuning_train_500_dataset.json\n",
      "   - Total ejemplos: 500\n",
      "   - 'No Duplicates': 231 (46.2%)\n",
      "   - 'Duplicates':    269 (53.8%)\n",
      "   - ✅ Dataset balanceado.\n",
      "\n",
      "============================================================\n",
      "PROCESANDO: fine_tuning_50_test_dataset.json\n",
      "============================================================\n",
      "Resultados del Análisis:\n",
      "Estructura, Template, Ningún error de duplicados internos. (Limpieza correcta)\n",
      "\n",
      "  [Distribución] Análisis del archivo: fine_tuning_50_test_dataset.json\n",
      "   - Total ejemplos: 53\n",
      "   - 'No Duplicates': 27 (50.9%)\n",
      "   - 'Duplicates':    26 (49.1%)\n",
      "   - ✅ Dataset balanceado.\n",
      "\n",
      " Verificando Fuga de Datos (Train vs Validation)...\n",
      "Limpio. No hay contaminación cruzada.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = process_dataset(FILE_TRAIN)\n",
    "val_dataset = process_dataset(FILE_VAL)\n",
    "if train_dataset and val_dataset:\n",
    "    check_cross_contamination(train_dataset, val_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine-tuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
